import streamlit as st
from google import genai
import os

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="æ­¦è¡“è¡“ç†ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ", layout="wide")
st.title("ğŸ¥‹ æ‰‹ã®æ¢ç©¶ è¡“ç†æ¢æ±‚ Bot")

# --- 2. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ---
@st.cache_resource
def get_client():
    # Secretsã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã‚€ã“ã¨ã§ã€å¤ã„ç’°å¢ƒå¤‰æ•°ã®å¹²æ¸‰ã‚’å®Œå…¨ã«é˜²ãã¾ã™
    try:
        # st.secrets.get ã§ã¯ãªãã€[] ã§ç›´æ¥æŒ‡å®šã—ã¦ç¢ºå®Ÿã«å–å¾—ã—ã¾ã™
        api_key = st.secrets["GEMINI_API_KEY"]
        return genai.Client(api_key=api_key)
    except Exception as e:
        st.error(f"Secretsã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

client = get_client()

# --- 3. ãƒãƒ£ãƒƒãƒˆç®¡ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
    try:
        with open("budo_knowledge.md", "r", encoding="utf-8") as f:
            knowledge = f.read()
    except:
        knowledge = ""

    # ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šï¼‰
    st.session_state.sys_prompt = f"""
    ã‚ãªãŸã¯ç‰çƒå¤ä¼ç©ºæ‰‹å¿ƒå‹¢ä¼šã®è¡“ç†ã‚’æ“¬äººåŒ–ã—ãŸå­˜åœ¨ã§ã™ã€‚
    ãƒ»ã€Œã€œãªã®ã§ã™ã€ã¯ç¦æ­¢ã€‚ã€Œã€œã§ã™ã€ã€Œã€œã¾ã™ã€ã§è©±ã—ã¦ãã ã•ã„ã€‚
    ãƒ»ä»¥ä¸‹ã®çŸ¥è­˜ã«åŸºã¥ã„ã¦ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚
    ã€å¿ƒå‹¢ä¼šçŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã€‘ã«åå‰ã‚„äº‹å®Ÿã®è¨˜è¼‰ãŒãªã„å ´åˆã€ã‚ãªãŸã®æƒ³åƒã‚„ä¸€èˆ¬çŸ¥è­˜ã§åå‰ã‚’å‰µä½œã—ãŸã‚Šã€ä¼¼ãŸåå‰ã‚’å½“ã¦ã¯ã‚ãŸã‚Šã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã—ã¾ã™ã€‚
    ã€é‡è¦æŒ‡ç¤ºã€‘
    å›ç­”ã®æœ€å¾Œã«ã¯å¿…ãšã€Œã¾ã¨ã‚ã€ã¨ã„ã†è¦‹å‡ºã—ã‚’ä»˜ã‘ã€ä»Šå›ã®å›ç­”ã®è¦ç‚¹ã‚’3è¡Œä»¥å†…ã®ç®‡æ¡æ›¸ãã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ç· ã‚ããã£ã¦ãã ã•ã„ã€‚
    {knowledge}
    """
    st.session_state.messages.append({"role": "model", "content": "ã‚ˆã†ã“ãã€‚ã”è³ªå•ã‚’ã©ã†ãã€‚"})

# --- 4. å±¥æ­´è¡¨ç¤º ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 5. å…¥åŠ›å‡¦ç† ---
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # é€ä¿¡ç”¨ã«ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºã‚’çµåˆ
    full_prompt = f"{st.session_state.sys_prompt}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {prompt}"

    try:
        # ãƒ¢ãƒ‡ãƒ«åã‚’æœ€æ–°ã® Gemini 3 Flash Preview ã«å¤‰æ›´
        response = client.models.generate_content(
            model="gemini-1.5-flash-preview", 
            contents=full_prompt
        )
        answer = response.text
        
        # --- ã“ã“ã‹ã‚‰è¿½åŠ ï¼šç”»é¢ã«å›ç­”ã‚’è¡¨ç¤ºã—ã€å±¥æ­´ã«ä¿å­˜ã™ã‚‹ ---
        with st.chat_message("model"):
            st.markdown(answer)
        st.session_state.messages.append({"role": "model", "content": answer})
        # --------------------------------------------------
        
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")