import streamlit as st
from google import genai
import os

# --- 1. ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="æ­¦è¡“è¡“ç†ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ", layout="wide")
st.title("ğŸ¥‹ å¿ƒå‹¢ä¼š è¡“ç†æ¢æ±‚ Bot")

# --- 2. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ---
@st.cache_resource
def get_client():
    # Secretsã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã‚€ã“ã¨ã§ã€å¤ã„ç’°å¢ƒå¤‰æ•°ã®å¹²æ¸‰ã‚’å®Œå…¨ã«é˜²ãã¾ã™
    try:
        # st.secrets.get ã§ã¯ãªãã€[] ã§ç›´æ¥æŒ‡å®šã—ã¦ç¢ºå®Ÿã«å–å¾—ã—ã¾ã™
        api_key = st.secrets["GEMINI_API_KEY"]
        return genai.Client(api_key=api_key)
    except Exception as e:
        st.error(f"Secretsã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

client = get_client()

# --- 3. ãƒãƒ£ãƒƒãƒˆç®¡ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
    try:
        with open("budo_knowledge.txt", "r", encoding="utf-8") as f:
            knowledge = f.read()
    except:
        knowledge = ""

    # ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šï¼‰
    st.session_state.sys_prompt = f"""
    ã‚ãªãŸã¯ç‰çƒå¤ä¼ç©ºæ‰‹å¿ƒå‹¢ä¼šã®ä»£è¡¨ã§ã™ã€‚
    ãƒ»ã€Œã€œãªã®ã§ã™ã€ã¯ç¦æ­¢ã€‚ã€Œã€œã§ã™ã€ã€Œã€œã¾ã™ã€ã§è©±ã—ã¦ãã ã•ã„ã€‚
    ãƒ»ä»¥ä¸‹ã®çŸ¥è­˜ã«åŸºã¥ã„ã¦ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚
    {knowledge}
    """
    st.session_state.messages.append({"role": "model", "content": "ã‚ˆã†ã“ãã€‚ã”è³ªå•ã‚’ã©ã†ãã€‚"})

# --- 4. å±¥æ­´è¡¨ç¤º ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 5. å…¥åŠ›å‡¦ç† ---
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # é€ä¿¡ç”¨ã«ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºã‚’çµåˆ
    full_prompt = f"{st.session_state.sys_prompt}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {prompt}"

    try:
        # ãƒ¢ãƒ‡ãƒ«åã‚’ 'models/' ã‹ã‚‰å§‹ã¾ã‚‹ãƒ•ãƒ«ãƒãƒ¼ãƒ ã«å¤‰æ›´ã—ã¾ã™
        # ã“ã‚Œã§ v1beta API ã§ã‚‚æ­£ã—ãèªè­˜ã•ã‚Œã¾ã™
        response = client.models.generate_content(
            model="models/gemini-1.5-flash", 
            contents=full_prompt
        )
        answer = response.text
        with st.chat_message("model"):
            st.markdown(answer)
        st.session_state.messages.append({"role": "model", "content": answer})
    except Exception as e:
        st.error(f"ã‚¨ãƒ©ãƒ¼ã®æ­£ä½“: {e}")